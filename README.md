# __Python Data Pipeline__

## __Overview__
- Data pipeline that processes all files in a folder
- Reads json files and combines them into one whilst also mapping columns from an external csv file
- In the folder `data`
  - `events_data-2020-07-01.json` and `events_data-2020-07-02.json`, which contain information about email traffic
  - `countries.csv`, which contain `country_code` to `country_name` mapping
- Finished product found in folder `output` as csv file

### main.py
- orchestrates data pipeline
- reads & outputs files

### utils.py
- where the data transformations occur
- combines all transformed data from each file into a single csv file

### Next steps
- improve utils.py to be more modular (more functions)
- load files to SQL database using SQLAlchemy(?)

### Output file contents
- `UID` is generated by anonymising the `to_address` field
- `geo_country_name` is generated by mapping the field `geo_country` using the file `countries.csv`
- `file_date` based on the `file_name` column, with format `YYYY-mm-dd`
- other columns are from source data; some have been removed i.e. `bounce_class`

## __How to execute code__
There are two main ways; linux terminal or PyCharm. I used Ubuntu 20.04 LTS on Windows to run main.py using `python3 main.py`.

__My setup:__
- Ubuntu 20.04 LTS on Windows
- python 3.8.10
- pandas 1.2.5
- glob
